{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5ef91f7",
      "metadata": {},
      "source": [
        "# HW07 — Кластеризация и внутренние метрики\n",
        "\n",
        "## Установка\n",
        "\n",
        "Для запуска ноутбука нужны:\n",
        "\n",
        "- `pandas`, `numpy`, `matplotlib`\n",
        "- `scikit-learn`\n",
        "\n",
        "Если используете `uv`, то зависимости ставятся так:\n",
        "\n",
        "```bash\n",
        "cd <корень-репозитория>\n",
        "uv sync\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878f9fa5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c51bf04",
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path('homeworks/HW07') if Path('homeworks/HW07').exists() else Path('.')\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "ARTIFACTS_DIR = BASE_DIR / 'artifacts'\n",
        "FIGURES_DIR = ARTIFACTS_DIR / 'figures'\n",
        "LABELS_DIR = ARTIFACTS_DIR / 'labels'\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LABELS_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ada4c5b",
      "metadata": {},
      "source": [
        "## 1. Датасеты и параметры\n",
        "\n",
        "Выбраны датасеты 01, 02 и 04.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "835809cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASETS = {\n",
        "    'ds1': {\n",
        "        'file': 'S07-hw-dataset-01.csv',\n",
        "        'k_range': list(range(2, 11)),\n",
        "        'dbscan_eps': [0.3, 0.5, 0.7, 1.0, 1.5],\n",
        "        'dbscan_min_samples': [5, 10],\n",
        "    },\n",
        "    'ds2': {\n",
        "        'file': 'S07-hw-dataset-02.csv',\n",
        "        'k_range': list(range(2, 11)),\n",
        "        'dbscan_eps': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0],\n",
        "        'dbscan_min_samples': [5, 10, 20],\n",
        "    },\n",
        "    'ds4': {\n",
        "        'file': 'S07-hw-dataset-04.csv',\n",
        "        'k_range': list(range(2, 11)),\n",
        "        'dbscan_eps': [0.5, 0.8, 1.0, 1.2, 1.5, 2.0, 2.5],\n",
        "        'dbscan_min_samples': [5, 10, 20],\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ee9a64",
      "metadata": {},
      "source": [
        "## 2. Вспомогательные функции\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63b1166",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_one_hot_encoder():\n",
        "    try:\n",
        "        return OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "def build_preprocessor(df):\n",
        "    feature_cols = [c for c in df.columns if c != 'sample_id']\n",
        "    X = df[feature_cols]\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == 'object']\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "    numeric_pipe = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler()),\n",
        "    ])\n",
        "    if cat_cols:\n",
        "        categorical_pipe = Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', make_one_hot_encoder()),\n",
        "        ])\n",
        "        preprocessor = ColumnTransformer([\n",
        "            ('num', numeric_pipe, num_cols),\n",
        "            ('cat', categorical_pipe, cat_cols),\n",
        "        ])\n",
        "    else:\n",
        "        preprocessor = ColumnTransformer([\n",
        "            ('num', numeric_pipe, num_cols),\n",
        "        ])\n",
        "    return preprocessor, X\n",
        "    \n",
        "def compute_metrics(X, labels):\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        return None\n",
        "    return {\n",
        "        'silhouette': float(silhouette_score(X, labels)),\n",
        "        'davies_bouldin': float(davies_bouldin_score(X, labels)),\n",
        "        'calinski_harabasz': float(calinski_harabasz_score(X, labels)),\n",
        "    }\n",
        "def compute_dbscan_metrics(X, labels):\n",
        "    noise_mask = labels == -1\n",
        "    noise_share = float(noise_mask.mean())\n",
        "    labels_non_noise = labels[~noise_mask]\n",
        "    X_non_noise = X[~noise_mask]\n",
        "    if len(np.unique(labels_non_noise)) < 2:\n",
        "        return None, noise_share\n",
        "    metrics = {\n",
        "        'silhouette': float(silhouette_score(X_non_noise, labels_non_noise)),\n",
        "        'davies_bouldin': float(davies_bouldin_score(X_non_noise, labels_non_noise)),\n",
        "        'calinski_harabasz': float(calinski_harabasz_score(X_non_noise, labels_non_noise)),\n",
        "    }\n",
        "    return metrics, noise_share"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b44959",
      "metadata": {},
      "source": [
        "## 3. Загрузка и первичный анализ (для каждого датасета)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "115bc2ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "for ds_key, cfg in DATASETS.items():\n",
        "    df = pd.read_csv(DATA_DIR / cfg['file'])\n",
        "    print('=' * 80)\n",
        "    print(ds_key, cfg['file'])\n",
        "    display(df.head())\n",
        "    display(df.info())\n",
        "    display(df.describe(include='all'))\n",
        "    missing = df.isna().sum().sort_values(ascending=False)\n",
        "    missing_share = (missing / len(df)).round(3)\n",
        "    display(pd.DataFrame({'missing': missing, 'share': missing_share}).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012de3aa",
      "metadata": {},
      "source": [
        "## 4. Обучение моделей, метрики и визуализация\n",
        "\n",
        "Для каждого датасета: KMeans (подбор k), DBSCAN (подбор eps и min_samples)\n",
        "Метрики: silhouette / Davies-Bouldin / Calinski-Harabasz,\n",
        "PCA(2D) для лучшего решения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cda468b",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_summary = {}\n",
        "best_configs = {}\n",
        "\n",
        "# тут основной цикл по датасетам\n",
        "for ds_key, cfg in DATASETS.items():\n",
        "    df = pd.read_csv(DATA_DIR / cfg['file'])\n",
        "    preprocessor, X_raw = build_preprocessor(df)\n",
        "    X = preprocessor.fit_transform(X_raw)\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print(f'{ds_key}: {cfg[\"file\"]}')\n",
        "\n",
        "    ds_metrics = {'kmeans': {}, 'dbscan': {}}\n",
        "\n",
        "    # KMeans: перебираем k\n",
        "    k_list = []\n",
        "    for k in cfg['k_range']:\n",
        "        model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        labels = model.fit_predict(X)\n",
        "        metrics = compute_metrics(X, labels)\n",
        "        ds_metrics['kmeans'][str(k)] = metrics\n",
        "        k_list.append((k, metrics['silhouette'] if metrics else -1))\n",
        "\n",
        "    ks = [k for k, _ in k_list]\n",
        "    sils = [v for _, v in k_list]\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(ks, sils, marker='o')\n",
        "    plt.title(f'{ds_key.upper()}: KMeans silhouette vs k')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('silhouette')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / f'{ds_key}_kmeans_silhouette_vs_k.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    best_k = max(k_list, key=lambda x: x[1])[0]\n",
        "\n",
        "    # DBSCAN: eps/min_samples\n",
        "    best_dbscan = None\n",
        "    for eps in cfg['dbscan_eps']:\n",
        "        for min_samples in cfg['dbscan_min_samples']:\n",
        "            model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = model.fit_predict(X)\n",
        "            metrics, noise_share = compute_dbscan_metrics(X, labels)\n",
        "            key = f'eps={eps},min_samples={min_samples}'\n",
        "            ds_metrics['dbscan'][key] = {'metrics': metrics, 'noise_share': noise_share}\n",
        "            if metrics is None or noise_share > 0.3:\n",
        "                continue\n",
        "            score = metrics['silhouette']\n",
        "            if best_dbscan is None or score > best_dbscan['score']:\n",
        "                best_dbscan = {\n",
        "                    'eps': eps,\n",
        "                    'min_samples': min_samples,\n",
        "                    'score': score,\n",
        "                    'metrics': metrics,\n",
        "                    'noise_share': noise_share,\n",
        "                }\n",
        "\n",
        "    best_k_metrics = ds_metrics['kmeans'][str(best_k)]\n",
        "    best_method = 'kmeans'\n",
        "    best_params = {'k': int(best_k)}\n",
        "    best_metrics = best_k_metrics\n",
        "    best_noise = None\n",
        "\n",
        "    if best_dbscan and best_dbscan['score'] > (best_k_metrics['silhouette'] if best_k_metrics else -1):\n",
        "        best_method = 'dbscan'\n",
        "        best_params = {'eps': best_dbscan['eps'], 'min_samples': best_dbscan['min_samples']}\n",
        "        best_metrics = best_dbscan['metrics']\n",
        "        best_noise = best_dbscan['noise_share']\n",
        "\n",
        "    # финальная модель, чтобы сохранить метки\n",
        "    if best_method == 'kmeans':\n",
        "        best_model = KMeans(n_clusters=best_params['k'], random_state=42, n_init=10)\n",
        "        labels = best_model.fit_predict(X)\n",
        "    else:\n",
        "        best_model = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "        labels = best_model.fit_predict(X)\n",
        "\n",
        "    labels_path = LABELS_DIR / f'labels_hw07_{ds_key}.csv'\n",
        "    pd.DataFrame({'sample_id': df['sample_id'], 'cluster_label': labels}).to_csv(labels_path, index=False)\n",
        "\n",
        "    # PCA для картинки\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=8, cmap='tab10', alpha=0.75)\n",
        "    plt.title(f'{ds_key.upper()}: PCA(2D) best ({best_method})')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / f'{ds_key}_pca_best.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    metrics_summary[ds_key] = {\n",
        "        'dataset_file': cfg['file'],\n",
        "        'kmeans': ds_metrics['kmeans'],\n",
        "        'dbscan': ds_metrics['dbscan'],\n",
        "        'best_method': best_method,\n",
        "        'best_metrics': best_metrics,\n",
        "        'best_noise_share': best_noise,\n",
        "    }\n",
        "\n",
        "    best_configs[ds_key] = {\n",
        "        'dataset_file': cfg['file'],\n",
        "        'best_method': best_method,\n",
        "        'best_params': best_params,\n",
        "        'selection_criterion': 'max silhouette (DBSCAN only if noise_share <= 0.3)',\n",
        "    }\n",
        "\n",
        "    print('Best method:', best_method)\n",
        "    print('Best params:', best_params)\n",
        "    print('Best metrics:', best_metrics)\n",
        "    if best_method == 'dbscan':\n",
        "        print('Noise share:', best_noise)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99e6b448",
      "metadata": {},
      "source": [
        "## 5. Устойчивость (KMeans, датасет ds1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "133a59ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = DATASETS['ds1']\n",
        "df = pd.read_csv(DATA_DIR / cfg['file'])\n",
        "preprocessor, X_raw = build_preprocessor(df)\n",
        "X = preprocessor.fit_transform(X_raw)\n",
        "\n",
        "best_k_ds1 = best_configs['ds1']['best_params'].get('k', 2)\n",
        "\n",
        "seeds = [0, 1, 2, 3, 4]\n",
        "labels_by_seed = []\n",
        "for seed in seeds:\n",
        "    model = KMeans(n_clusters=int(best_k_ds1), random_state=seed, n_init=10)\n",
        "    labels_by_seed.append(model.fit_predict(X))\n",
        "\n",
        "pairwise_ari = []\n",
        "for i in range(len(seeds)):\n",
        "    for j in range(i + 1, len(seeds)):\n",
        "        pairwise_ari.append(adjusted_rand_score(labels_by_seed[i], labels_by_seed[j]))\n",
        "\n",
        "stability = {\n",
        "    'dataset': 'ds1',\n",
        "    'kmeans_k': int(best_k_ds1),\n",
        "    'seeds': seeds,\n",
        "    'pairwise_ari_mean': float(np.mean(pairwise_ari)),\n",
        "    'pairwise_ari_min': float(np.min(pairwise_ari)),\n",
        "    'pairwise_ari_max': float(np.max(pairwise_ari)),\n",
        "}\n",
        "\n",
        "stability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6fe9dfc",
      "metadata": {},
      "source": [
        "## 6. Сохранение артефактов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12f1908",
      "metadata": {},
      "outputs": [],
      "source": [
        "(ARTIFACTS_DIR / 'metrics_summary.json').write_text(json.dumps(metrics_summary, indent=2))\n",
        "(ARTIFACTS_DIR / 'best_configs.json').write_text(json.dumps(best_configs, indent=2))\n",
        "(ARTIFACTS_DIR / 'stability_kmeans_ds1.json').write_text(json.dumps(stability, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43d1582",
      "metadata": {},
      "source": [
        "## 7. Итоговые выводы по датасетам\n",
        "\n",
        "**ds1 (dataset-01)**\n",
        "- После масштабирования KMeans с небольшим числом кластеров дал лучший silhouette. DBSCAN работал стабильнее при большем eps, уступает по метрикам. Явные различия масштабов критичны: без scaling результат заметно хуже.\n",
        "\n",
        "**ds2 (dataset-02)**\n",
        "- Нелинейная структура лучше выделилась через DBSCAN при eps=1.0 и min_samples=20. При корректном eps шум минимален, а silhouette заметно выше, чем у KMeans. KMeans чувствителен к форме кластеров, дает срез для нелинейных групп.\n",
        "\n",
        "**ds4 (dataset-04)**\n",
        "- Использовался имьютинг числовых и one-hot для категориальных признаков. DBSCAN с eps=2.5 и min_samples=20 дал допустимыый баланс метрик и шума. Высокая размерность снижает контраст кластеров, метрики умеренные.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}